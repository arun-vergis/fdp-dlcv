{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch vs Tensorflow\n",
    "[Pytorch](https://pytorch.org/) and [Tensorflow](https://www.tensorflow.org/) are both open-source deep-learning frameworks\n",
    "\n",
    "Tensorflow, based on Theano is Google’s brainchild born in 2015\n",
    "\n",
    "PyTorch, is a close cousin of Lua-based Torch framework born out of  Facebook’s AI research lab in 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch dominates in Research\n",
    "[Conference Publications](http://horace.io/pytorch-vs-tensorflow/)\n",
    "\n",
    "[paperswithcode](https://paperswithcode.com/)\n",
    "\n",
    "[Stanford's NLP course](http://web.stanford.edu/class/cs224n/)\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) Much more models are in Pytorch\n",
    "\n",
    "[SpeechBrain](https://speechbrain.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow dominates in Production\n",
    "[Stackoverflow Developer Survey 2021](https://insights.stackoverflow.com/survey/2021#section-most-popular-technologies-other-frameworks-and-libraries)\n",
    "\n",
    "[Tensroflow.js](https://www.tensorflow.org/js) makes it possible to develop and train machine learning models using Javascript.\n",
    "\n",
    "[Tensorflow Lite](https://www.tensorflow.org/lite) allows model deployment on mobile and embedded devices-Google's [Edge TPUs](https://cloud.google.com/edge-tpu) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Search trends](https://trends.google.com/trends/explore?date=today%205-y&q=pytorch,tensorflow)\n",
    "\n",
    "You’re not making a mistake in choosing either framework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Pytorch\n",
    "Create a new conda environment [Managing conda environments:](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)\n",
    "\n",
    "\n",
    "`conda create --name myenv --clone base`\n",
    "\n",
    "`conda activate myenv`\n",
    "\n",
    "Then install pytorch in `myenv` by copy-pasting the appropriate command from [pytorch.org](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "Check installation by importing torch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "A Tensor is the fundamental data structure in Pytorch used to store and\n",
    "manipulate data. Let us create some tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.]) # 1D tensor \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSZQ1WkIL9ih"
   },
   "source": [
    "Every tensor is a grid of elements of the **same type**. Pytorch provides a large set of numeric datatypes that you can use to construct tensors. Pytorch tries to guess a datatype when you create a tensor, but we can also specify the datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4za4O0m5L9ih",
    "outputId": "2ea4fb80-a4df-43f9-c162-5665895c13ae"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2])  # Let pytorch choose the datatype\n",
    "y = torch.tensor([1.0, 2.0])  # Let pytorch choose the datatype\n",
    "z = torch.tensor([1, 2], dtype=torch.float16)  # Force a particular datatype\n",
    "\n",
    "print(x.dtype, y.dtype, z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When operating with arrays of different types, the type of the resulting array corresponds to the more general or precise one (a behavior known as upcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = x + y\n",
    "w.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of type coercion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a float into an int tensor truncates the decimal part\n",
    "x[0] = 10.6\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-dimensional Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2., 3.], [ 4., 5., 6.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor creation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.full((2,3), 7.) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(2,3) # uninitialized\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand(size): random numbers [0, 1)\n",
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 3)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(10, 20, (5, 3)) #randint(low, high, size)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros_like(x)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0., 1. , 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to `arange` is `linspace`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = torch.linspace(0., np.pi, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical functions apply to entire tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be plotted using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical operations are performed element wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2], [3, 4]])\n",
    "y = torch.tensor([[5, 6], [7, 8]])\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division\n",
    "x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x + torch.tensor([2,3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be indexed, sliced and iterated over, much Python like lists and Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10.) # create a 1D tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing\n",
    "a[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual value when your tensor has only a single element, use `item()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[2:5] #slicing - start:stop:step\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slice of a tensor is a view into the same data, so modifying it will modify the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0] = 99 # modifying b modifies a also\n",
    "a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a copy instead of a view, you should use the `clone` method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.clone()\n",
    "c[-1] = 100\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional tensors can also be indexed and sliced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wLWA0udwL9iD",
    "outputId": "99f08618-c513-4982-8982-b146fc72dab3"
   },
   "outputs": [],
   "source": [
    "a = torch.arange(25).reshape(5, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYv4JyIEL9iD"
   },
   "source": [
    "When slicing multidimensional tensors, you must specify a slice for each dimension of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1: , 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reshaping, we can use `torch.reshape` or `torch.view()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4, 3)\n",
    "x.view(2,6) # - no memory copy happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1, 6)  # the size -1 is inferred from other dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3., 4.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(1, 4) # y is a 2D array\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x to a column tensor z\n",
    "z = x.view(4,1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose using .T or .transpose()\n",
    "w = z.T\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many Python libraries, images are stored as `[height, width, channels]` ,\n",
    "but PyTorch prefers to deal with these in a `[channels, height, width]`. You can\n",
    "use `permute()` to deal with these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(4, 5, 3)\n",
    "img_t = img.permute(2, 0, 1)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks processes images in batches for efficiency. Pytorch expects batches of images in `[batches, channels, height, width]`. If we have only a single image, we can convert it into a batch of one image using `unsqueeze`. `unsqueeze()` adds dimension of size 1 at indicated postion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`squeeze()` returns a tensor with all the dimensions of the input tensor of size 1 removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperability with Numpy\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is very easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both objects will share the same memory location, so changing one will also change the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_(1)  #everything with a trailing underscore is an inplace operation\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy array to Torch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.tensor(a) # also, b = from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again be careful when modifying\n",
    "a += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Example\n",
    "The ability to accelerate tensor operations on a GPU is a\n",
    "major advantage of tensors over NumPy arrays,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default all tensors are created on the CPU,\n",
    "# but you can also move them to the GPU (if it's available )\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "x = x.to(device) # or just use string ``x.to(\"cuda\")``\n",
    "\n",
    "\n",
    "y = torch.ones((2, 2), device = device)  # directly create a tensor on GPU\n",
    "\n",
    "z = x + y\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.numpy() # not possible because numpy cannot handle GPU tenors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to CPU again\n",
    "w = z.to('cpu').numpy() \n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "PyTorch tensors can remember where they come from, in terms of the operations and\n",
    "parent tensors that originated them, and they can automatically provide the chain of\n",
    "derivatives of such operations with respect to their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the derivate of the function $ y=x^2 $ at $x=2$ using Pytorch. ie. $\\frac{dy}{dx}|_{x=2}$\n",
    "\n",
    "Let us create the tensor <code>x</code> and set the parameter <code>requires_grad</code> to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True) # requires_grad = True -> tracks all operations on the tensor. \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `requires_grad` argument\n",
    "is telling PyTorch to track the entire family tree of tensors resulting from operations\n",
    "on `x`. In other words, we can automatically compute the derivative of any tensor that will have `x` as an ancestor. \n",
    "The value of the derivative will be automatically populated as a `grad` attribute of the\n",
    "`x` tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us create a tensor y according to $ y=x^2 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us take the derivative with respect x at x = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto gradient calculation does not reset the gradients automatically. It accumulates by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to zero the gradient explicitly using `.grad.zero_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all tensors with `requires_grad=True` are tracking their\n",
    "computational history and support gradient computation. However, there\n",
    "are some cases when we do not need to do that. We can stop\n",
    "tracking computations by surrounding our computation code with\n",
    "`torch.no_grad()` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2\n",
    "print(y.requires_grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve the same result is to use the ``detach()`` method\n",
    "on the tensor:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the derivative of $ y = 2x^3+x $ at $x=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Calculate the derivative of y = 2x^3 + x at x = 1\n",
    "\n",
    "# Type your code here\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = 2*x**3 + x\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- \n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = 2 * x ** 3 + x\n",
    "y.backward()\n",
    "print(\"The derivative result: \", x.grad)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate <b>Partial Derivatives</b>. Consider the function: $f(u,v)=vu+u^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create <code>u</code> tensor, <code>v</code> tensor and  <code>f</code> tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n",
    "\n",
    "u = torch.tensor(1.0,requires_grad=True)\n",
    "v = torch.tensor(2.0,requires_grad=True)\n",
    "f = u * v + u ** 2\n",
    "print(\"The result of v * u + u^2: \", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(u=1,v=2)=(2)(1)+1^{2}=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `.backward` on `f`, Pytorch's automatic differentiation engine computes the gradients wrt `u` and `v` and stores in the variable's `.grad` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take the derivative with respect to <code>u</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The partial derivative with respect to u: \", u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the expression is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {u}}=v+2u$\n",
    "\n",
    "$\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {u}}=2+2(1)=4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take the derivative with respect to <code>v</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The partial derivative with respect to u: \", v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {v}}=u$\n",
    "\n",
    "$\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {v}}=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to determine partial derivative  $u$ of the following function where $u=2$ and $v=1$: $ f=uv+(uv)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Calculate the derivative of f = u * v + (u * v) ** 2 at u = 2, v = 1\n",
    "\n",
    "# Type the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "u = torch.tensor(2.0, requires_grad = True)\n",
    "v = torch.tensor(1.0, requires_grad = True)\n",
    "f = u * v + (u * v) ** 2\n",
    "f.backward()\n",
    "print(\"The result is \", u.grad)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More details on `torch.autograd` [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#a-gentle-introduction-to-torch-autograd) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regesssion Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
    "f = 2 * X - 3\n",
    "# Add noise\n",
    "Y = f + 0.2 * torch.randn(X.size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the line and the points with noise\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(X, Y, 'ro', label = 'y')\n",
    "plt.plot(X, f, label = 'f')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "From the data, we observe that `y` is linearly related to `x`. So we decide to model `y` as a linear function of `x`. \n",
    "$y = w*x + b $\n",
    "\n",
    "To create a simple linear regression model to predict `y` from `x`, we feed our training\n",
    "examples, and the regression algorithm learns the parameters that make the linear model fit best to your data. This is called training the model. In our example, the parameters to be determined are the slope and intercept of the best-fit line.  Then we can make predictions using the `forward` function as `yhat = forward(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward function\n",
    "\n",
    "def forward(x, w, b):\n",
    "    return w * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "Before we can fit data with our model, we need to determine a measure of *fitness*. The loss function quantifies the distance between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better. The most popular loss function in regression problems is the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MSE Loss function\n",
    "\n",
    "def criterion(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss surface to visualize the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.linspace(-15, 15, 30)\n",
    "B = np.linspace(-15, 15, 30)\n",
    "Z = np.empty((len(W), len(B)))\n",
    "for i, bi in enumerate(B):\n",
    "    for j, wj in enumerate(W):\n",
    "        Yhat=forward(X, wj, bi)\n",
    "        Z[i, j] = criterion(Yhat, Y)\n",
    "W, B = np.meshgrid(W, B)\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.axes(projection='3d').plot_surface(W, B, Z, cmap = 'viridis', edgecolor = 'none')\n",
    "plt.title('Loss Surface')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.title('Loss Surface Contour')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.contour(W, B, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model parameters <code>w</code>, <code>b</code> by setting the argument <code>requires_grad</code> to True because we must learn it using the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial values of the parameters w, b for y = wx + b\n",
    "\n",
    "w = torch.tensor(-15.0, requires_grad = True)\n",
    "b = torch.tensor(-10.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the learning rate to 0.1 and create an empty list `history` for storing the loss for each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate and create empty lists for saving the loss and parameter values at each iteration.\n",
    "\n",
    "lr = 0.1\n",
    "loss_history = []\n",
    "w_history=[]\n",
    "b_history=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 10\n",
    "for epoch in range(iter):\n",
    "\n",
    "\n",
    "        # make a prediction\n",
    "        Yhat = forward(X, w, b )\n",
    "        \n",
    "        # calculate the loss \n",
    "        loss = criterion(Yhat, Y)\n",
    "    \n",
    "        # store the loss in the list LOSS\n",
    "        loss_history.append(loss.item())\n",
    "        w_history.append(w.item())\n",
    "        b_history.append(b.item())\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters slope and bias\n",
    "        w.data = w.data - lr * w.grad.data\n",
    "        b.data = b.data - lr * b.grad.data\n",
    "        \n",
    "        # zero the gradients before running the backward pass\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss for each iteration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out the Loss Result\n",
    "plt.plot(loss_history)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Epoch/Iterations\")\n",
    "plt.ylabel(\"Cost\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.title('Loss Surface Contour')\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('w')\n",
    "plt.contour(W, B, Z)\n",
    "plt.plot(b_history, w_history, 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression as a single Neuron model\n",
    "We can think of a linear regression model as a neural network consisting of just a single artificial neuron.\n",
    "Simple linear regression: only one input x\n",
    "![neuron](./neuron.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With neural networks, we do not have to fix the shape of the function as linear, quadratic,...\n",
    "ANNs are capable of representing complicated functions through a composition of simpler functions. As the dimensionality of\n",
    "a problem grows (that is, many inputs to many outputs) and input/output relationships\n",
    "get complicated, assuming a shape for the input/output function is unlikely to\n",
    "work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch `nn` module\n",
    "Contains the building blocks needed to create all sorts of neural network architectures.\n",
    "\n",
    "`nn.Linear` sub-module, applies\n",
    "an affine transformation to its input (via the parameter attributes weight and bias)\n",
    "and is equivalent to what we implemented earlier\n",
    "\n",
    "We can create an instance of `nn.Linear` by passing three arguments: *the\n",
    "number of input features, the number of output features, and whether the linear\n",
    "model includes a bias or not (defaulting to True)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "model = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a single neuron. We can also create a layer using nn.Linear. The number of neurons in a layer = number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_3outs = nn.Linear(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the instance as if it were a function `output = model(input)`. Pytorch nn modules are designed to accept multiple samples at a time. To accommodate multiple samples, modules expect the zeroth dimension\n",
    "of the input to be the number of samples in the batch. Thus, assuming we need to run nn.Linear on 10 samples, we can create an\n",
    "input tensor of size 10 × Nin, where Nin is the number of\n",
    "input features, and run it once through the model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs  = torch.ones(5,1)\n",
    "outputs =  model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight, model.bias #requires_grad=True since the parameters need to be optimized by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_3outs.weight, mdl_3outs.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mdl_3outs.parameters()) # we dont have to create parameters on our own\n",
    "#list(model.named_parameters()) gives the name too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to write our own loss function as `nn` comes with several common loss functions, among them `nn.MSELoss`. \n",
    "We have to create an instance and call it as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also don't have to write our own optimization algorithm for parameter updates.  The torch\n",
    "module has an `optim` submodule where we can find classes implementing different\n",
    "optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    Yhat = model(X) #model accepts multiple samples\n",
    "    loss = loss_fn(Yhat, Y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() #we dont have to worry abt updating each and every param. optimizer abstracts away the details- it got the params and lr\n",
    "    print(f'Epoch:{epoch}, Loss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight, model.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
