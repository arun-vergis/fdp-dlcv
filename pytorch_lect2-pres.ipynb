{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification using NNs\n",
    "\n",
    "The first stage of deep-learning development is data preparation where we acquire data to train and test our models.\n",
    "\n",
    "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html), all of which include datasets. \n",
    "\n",
    "The ``torchvision.datasets`` module contains many real-world vision data like MNIST, Fashion-MNIST, CIFAR, COCO,... ([full list here](https://pytorch.org/vision/stable/datasets.html)). \n",
    "\n",
    "Here we will be using the Fashion-MNIST dataset. Fashion MNIST is a drop-in replacement of MNIST. It has the exact same\n",
    "format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits.\n",
    "Of the 70,000 images, 60,000 are training examples and 10,000 are test examples.\n",
    "\n",
    "The dataset object returns the data in the form of Python Imagine Library (PIL) images, which we convert to tensors by passing a `transform=ToTensor()` parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26422272it [00:34, 760111.16it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 30693457.76it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4422656it [00:00, 12163495.90it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6144it [00:00, 393949.37it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor() # NN needs tensors as input; this transform is applied whenver the dataset is accessed\n",
    "   \n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training samples:', len(train_data))\n",
    "print('Test samples:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Dataset\n",
    "\n",
    "We can index ``Datasets`` manually like a list: ``train_data[index]``. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor, label = train_data[0] # returns a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, int)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_tensor), type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape  # [channels, height, width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pixel intensities of the images are represented by floating-point values in between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min intensity value:  0.0\n",
      "Max intensity value:  1.0\n"
     ]
    }
   ],
   "source": [
    "# conversion from uint8 to float happens as part of ToTensor()\n",
    "\n",
    "print('Min intensity value: ', img_tensor.min().item())\n",
    "print('Max intensity value: ', img_tensor.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use matplotlib to visualize the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_tensor.squeeze(), cmap='gray') # or img_tensor.view(28,28)\n",
    "plt.axis(\"off\")\n",
    "plt.title(train_data.classes[label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, when training and testing the model, we will pass small **batches** of data at each iteration, rather than one sample at a time. Sending data in batches not only allows more efficient training but also takes advantage of the parallel nature of GPUs. PyTorch has an object called **DataLoader** that can create batches of our data for us automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader is an iterable\n",
    "#  We pass the dataset as an argument to the dataloader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fully-connected neural networks\n",
    "\n",
    "\n",
    "Fashion-MNIST is a classification problem. We will start with the simplest possible approach for image classification - a fully-connected neural network\n",
    "\n",
    "\n",
    "A basic **neural network** in PyTorch consists of a number of **layers**. The simplest network would include just one fully-connected layer, which is called **Linear** layer, with 784 inputs (one input for each pixel of the input image) and 10 outputs (one output for each class).\n",
    "\n",
    "![A graph showing how an image is broken into layers based on the pixels.](./fig1.png)\n",
    "\n",
    "\n",
    "It can be defined in PyTorch in the following way, using `Sequential` syntax. \n",
    "As we discussed above, the dimension of our images is $1\\times28\\times28$. Because the input dimension of a fully-connected layer is 784, \n",
    "we need to insert another layer into the network, called **Flatten**, to change tensor shape from $1\\times28\\times28$ to $784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "        nn.Flatten(), \n",
    "        nn.Linear(784,10), # 784 inputs, 10 outputs\n",
    "        nn.LogSoftmax(dim=1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered \n",
    "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
    "sequential containers to put together a quick network\n",
    "\n",
    "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) \n",
    "layer converts each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained).\n",
    "\n",
    "[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "is a module that applies a linear transformation on the input using it's stored weights and biases.\n",
    "\n",
    "We want $n$-th output of the network to return the probability of the input image belonging to class $n$. Because the output of a fully-connected layer\n",
    "is not normalized to be between 0 and 1, it cannot be thought of as probability. To turn it into a probability we need to apply another layer called\n",
    "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "In PyTorch, it is easier to use [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html), which will also compute logarithms of output probabilities. \n",
    "\n",
    "To turn the output vector into the actual probabilities, we need to take `torch.exp()` of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus, the architecture of our network can be represented by the following sequence of layers:\n",
    "\n",
    "![An image showing the architecture of the network broken into a sequence of layers.](./onelayer-network-layers-fashion-mnist1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
      "  (2): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Class Probabilities\n",
    "\n",
    "A network defined this way can take images as input and produce a vector of probabilities as an output for each image. Let's see how this network performs by giving it a batch of 16 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28]) torch.Size([16, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0886, 0.0872, 0.0667, 0.0861, 0.0825, 0.1282, 0.0936, 0.0981, 0.1848,\n",
       "         0.0843],\n",
       "        [0.1541, 0.0855, 0.0594, 0.0892, 0.1056, 0.0951, 0.1060, 0.1089, 0.1216,\n",
       "         0.0745],\n",
       "        [0.1016, 0.0912, 0.0968, 0.0883, 0.1123, 0.0992, 0.0931, 0.0917, 0.1278,\n",
       "         0.0981],\n",
       "        [0.1072, 0.0960, 0.0919, 0.0885, 0.1280, 0.1027, 0.0922, 0.0998, 0.1073,\n",
       "         0.0862],\n",
       "        [0.1029, 0.0880, 0.0816, 0.0758, 0.1260, 0.0925, 0.0912, 0.1024, 0.1396,\n",
       "         0.0999],\n",
       "        [0.1112, 0.0895, 0.0774, 0.0611, 0.1015, 0.1096, 0.1130, 0.1266, 0.1347,\n",
       "         0.0753],\n",
       "        [0.0737, 0.0976, 0.0981, 0.0911, 0.0872, 0.1186, 0.1190, 0.1285, 0.0942,\n",
       "         0.0921],\n",
       "        [0.1055, 0.0857, 0.0597, 0.0436, 0.0991, 0.1147, 0.0919, 0.1440, 0.1935,\n",
       "         0.0622],\n",
       "        [0.0779, 0.1032, 0.0837, 0.1119, 0.1009, 0.0994, 0.1063, 0.1092, 0.1118,\n",
       "         0.0957],\n",
       "        [0.1044, 0.1186, 0.0795, 0.0949, 0.0960, 0.1170, 0.0943, 0.0994, 0.1161,\n",
       "         0.0798],\n",
       "        [0.1266, 0.0865, 0.0767, 0.0893, 0.1185, 0.0874, 0.0992, 0.1080, 0.1296,\n",
       "         0.0782],\n",
       "        [0.0799, 0.1138, 0.0704, 0.1214, 0.0975, 0.1129, 0.1097, 0.0844, 0.1310,\n",
       "         0.0790],\n",
       "        [0.0889, 0.1031, 0.0878, 0.0831, 0.0941, 0.1250, 0.0949, 0.1047, 0.1137,\n",
       "         0.1046],\n",
       "        [0.0861, 0.0941, 0.0764, 0.0923, 0.1047, 0.1092, 0.1070, 0.1267, 0.1067,\n",
       "         0.0969],\n",
       "        [0.0817, 0.0982, 0.0948, 0.0869, 0.0837, 0.1154, 0.1115, 0.1261, 0.0942,\n",
       "         0.1075],\n",
       "        [0.0811, 0.1043, 0.0799, 0.0718, 0.0898, 0.1132, 0.0990, 0.1236, 0.1558,\n",
       "         0.0816]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to use iter() to cast the trainloader to an iterator\n",
    "# and then use next() to iterate over the data one more time.\n",
    "# This is only necessary when accessing one batch. As we’ll see\n",
    "# later, our training loops will access the dataloader directly\n",
    "# without the need for iter() and next().\n",
    "\n",
    "inputs, labels = next(iter(train_loader))\n",
    "outputs = model(inputs)\n",
    "print(inputs.shape, outputs.shape)\n",
    "torch.exp(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the network predicts similar probabilities for each class. This is because it has not been trained on how to recognize the classes. We need to give it our training data to train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network\n",
    "\n",
    "The training process steps are as follows:\n",
    "\n",
    "1. We take a minibatch from the input dataset, which consists of input images and expected result (label).\n",
    "2. We calculate the predicted result for this minibatch. \n",
    "3. The difference between this result and expected result is calculated using a special function called the **loss function**.  In most of the classification tasks `NLLLoss` is used.\n",
    "4. We calculate the gradients of this loss function with respect to model weights (parameters), which are then used to adjust the weights to optimize the performance of the network. The amount of adjustment is controlled by a parameter called **learning rate**.  **Learning rate** defines the speed at which the network learns. If the learning rate is too high, new values will overwrite the knowledge from the old ones, and the network would perform unpredictably. If the learning rate is too small it results in a very slow learning process. The details of optimization algorithm are defined in the **optimizer** object. The most traditional algorithm is *stochastic gradient descent*, but we will use a more advanced version called **Adam** \n",
    "5. We repeat those steps until the whole dataset is processed. One complete pass through the dataset is called **an epoch**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The loss function determines how we measure the performance of our model and\n",
    "# computes the loss or error between predictions and truth. We’ll\n",
    "# attempt to minimize the loss by adjusting the model parameters\n",
    "# during training. The optimizer defines how we update our\n",
    "# model’s parameters during training\n",
    "\n",
    "\n",
    "# To define the loss function and the optimizer, we use the\n",
    "# `torch.optim` and `torch.nn` packages :\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), # PyTorch optimizers require that you pass in the model parameters using the parameters() method\n",
    "                      lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    " * Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent accumulation, we explicitly zero them at each iteration.\n",
    " * Back-propagate the prediction loss with a call to `loss.backward()`. PyTorch computes the gradients of the loss w.r.t. each parameter. \n",
    " * Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following PyTorch code demonstrates the fundamental training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=5\n",
    "for epoch in range(n_epochs): # \n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        outputs = model(inputs) # \n",
    "        loss = loss_fn(outputs, labels) # \n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # \n",
    "        loss.backward() #\n",
    "        optimizer.step() #\n",
    "\n",
    "        epoch_loss += loss.item() #\n",
    "    print(f'Epoch:{epoch+1} Loss:{epoch_loss/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we do when training:\n",
    "* For each epoch:\n",
    "* Go over all batches in the dataset, and for each batch (`inputs`) do the following:\n",
    "   - compute predictions made by the network on this batch (`outputs`)\n",
    "   - compute `loss`, which is the discrepancy between predicted and expected values\n",
    "   - try to minimize the loss by adjusting weights of the network (`optimizer.step()`)\n",
    "   \n",
    "   \n",
    "By observing the loss during training we can see whether the network is improving and learning from the data provided.\n",
    "\n",
    "Now that we have trained our model and attempted to minimize\n",
    "the loss, how can we evaluate its performance? How do\n",
    "we know that our model will generalize and work with data it\n",
    "has never seen before?\n",
    "\n",
    "Model development often includes validation\n",
    "to ensure that overfitting does not occur and that the model\n",
    "will perform well against unseen data. Typically, we will reserve a portion of the training data for validation.\n",
    "The validation data will not be used to train the NN;\n",
    "instead, we’ll use it to test the performance of the model at the\n",
    "end of each epoch. \n",
    "\n",
    "Before we perform validation, we need to split our training\n",
    "dataset into a training dataset and a validation dataset, as\n",
    "shown in the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_set, val_set = random_split(\n",
    "                      train_data, \n",
    "                      [50000, 10000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size=16)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the previous fundamental training\n",
    "example with validation added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the network to start from scratch\n",
    "model = nn.Sequential(\n",
    "        nn.Flatten(), \n",
    "        nn.Linear(784,10), # 784 inputs, 10 outputs\n",
    "        nn.LogSoftmax(dim=1))\n",
    "        \n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} Train Loss: {train_loss/len(train_loader):.3f} Val Loss: {val_loss/len(val_loader):.3f}\")\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this might seem a little complicated process, but in real life, we need to write this code once as a function , and then reuse the function when needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, train_loader, val_loader, optimizer = None, lr = 0.001, n_epochs=5, loss_fn = nn.NLLLoss()):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optimizer or torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    res = { 'train_loss' : [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training \n",
    "        running_loss , num_correct = 0.0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _,predicted = torch.max(outputs,1)\n",
    "            num_correct += (predicted == labels).sum().item()           \n",
    "\n",
    "        train_loss = running_loss/len(train_loader) \n",
    "        train_acc = num_correct/len(train_loader.dataset) \n",
    "        res['train_loss'].append(train_loss)        \n",
    "        res['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        running_loss, num_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _,predicted = torch.max(outputs,1)\n",
    "                num_correct += (predicted == labels).sum().item()            \n",
    "\n",
    "            val_loss = running_loss/len(val_loader) \n",
    "            val_acc = num_correct/len(val_loader.dataset) \n",
    "            res['val_loss'].append(val_loss)        \n",
    "            res['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:2}, Train acc={train_acc:.3f}, Val acc={val_acc:.3f}, Train loss={train_loss:.3f}, Val loss={val_loss:.3f}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the network to start from scratch\n",
    "model = nn.Sequential(\n",
    "        nn.Flatten(), \n",
    "        nn.Linear(784,10), # 784 inputs, 10 outputs\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "history=train_val(model, train_loader,val_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function logs messages with the accuracy on training and validation data from each epoch. It also returns this data as a dictionary (called **history**). We can then visualize this data to better understand our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history['train_acc'], label='Training acc')\n",
    "    plt.plot(history['val_acc'], label='Validation acc')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history['train_loss'], label='Training loss')\n",
    "    plt.plot(history['val_loss'], label='Validation loss')\n",
    "    plt.legend()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs indicate that the model is **overfitted**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "The model has never seen the test data during training, nor has\n",
    "the test data had any influence on the hyperparameters. Let’s\n",
    "see how we perform against a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model(test_data[0][0].to(device))\n",
    "print('Class Probabilites:', torch.exp(output))\n",
    "print('Predicted Class:', torch.argmax(output, dim=1).item())\n",
    "print('True Class:', test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test on the entire test set, we use the `DataLoader` class to create iterable batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor() \n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16) # we can use larger batch size for testing\n",
    "test_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "num_correct = 0\n",
    "for inputs, labels in test_loader:\n",
    "    labels = labels.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    num_correct += (predicted == labels).float().sum()\n",
    "\n",
    "accuracy = num_correct/len(test_loader.dataset)\n",
    "\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing network weights\n",
    "\n",
    "Now let's visualize our weights of our neural network and see what they look like. When the network is more complex than just one layer it can be a difficult to visulize the results like this. However, in our case it happens by multiplying the initial image by a weight matrix allowing us to visualize the network weights with a bit of added logic.\n",
    "\n",
    "This results in the weight tensor elements somewhat resembling the average shape of the item it classifies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "weight_tensor = next(model.parameters()) #10x784 tensor\n",
    "fig,ax = plt.subplots(1,10,figsize=(15,4))\n",
    "for i,x in enumerate(weight_tensor):\n",
    "    ax[i].imshow(x.view(28,28).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a single-layer dense neural network shows relatively good performance, but we want to get higher than 84% on accuracy! Let us see if we can get better performance by adding more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "In a multi-layer network, we will add one or more **hidden layers**.\n",
    "\n",
    "![An image showing a multi-layer network with a hidden layer between the input layer and the output layer.](./dense-multilayer-network-fashionmnist-small.png)\n",
    "\n",
    "This layer may contain any number of neurons, which will affect how powerful our neural network it, i.e. how many parameters will it have. The more parameters there are in the network - the more data we need to train it.\n",
    "\n",
    "However, _more_ is not always _better_. A number of parameters of a neural network should be chosen depending on the dataset size, to prevent **overfitting**.\n",
    "\n",
    "Our network layer structure will look like this:\n",
    "\n",
    "![An image showing the network layer structure as it's broken down into layers.](./multilayer-network-layers-fashionmnist-small.png)\n",
    "\n",
    "An important thing to note here is the non-linear activation function layer, called **ReLU**. It is important to introduce those non-linear activation functions, because they are one of the reasons neural networks achieve high expressive power. Indeed, it can be demonstrated mathematically that if a network consisted just of a series of linear layers, it would essentially be equivalent to one linear layer. Thus inserting non-linear functions in between layers is important!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network can be defined in PyTorch in the following way, using `Sequential` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (4): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "net2 = nn.Sequential(\n",
    "        nn.Flatten(), \n",
    "        nn.Linear(784,100),     # 784 inputs, 100 outputs\n",
    "        nn.ReLU(),              # Activation Function\n",
    "        nn.Linear(100,10),      # 100 inputs, 10 outputs\n",
    "        nn.LogSoftmax(dim=1))\n",
    "\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train_val(net2, train_loader, val_loader, n_epochs=10)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway\n",
    "\n",
    "Multi-level networks can achieve higher accuracy than single-layer perceptron, however, they are not perfect for computer vision tasks. In images, there are some structural patterns that can help us classify an object regardless of it's position in the image, but perceptrons do not allow us to extract those patterns and look for them selectively. Convolutional Neural Networks(CNNs) and Vision Transformers(ViTs) are more effective for computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-based network definitions\n",
    "\n",
    "Defining models using a `Sequential` style as a list of layers seems very convenient but it is somewhat limited. PyTorch provides tremendous flexibility to a programmer about how to create, combine, and process tensors as they flow through a network (called computational graph) paired with a relatively high-level, object-oriented API.  This means we need to utilize a little bit of object oriented programming (OOP) in Python.\n",
    "\n",
    "[Primer on object oriented programming in Python\n",
    "](./intro_oop-pres.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing `nn.Module`, and \n",
    "initialize the neural network layers in `__init__`. The operations on input data are defined in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(784,100)\n",
    "        self.out = nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (hidden): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MyNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the structure of a neural network is the same as with the `Sequential`-defined network, but the definition is more explicit. Our custom neural network is represented by a class inherited from [``torch.nn.Module``](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class.\n",
    "\n",
    "Class definition consists of two parts:\n",
    "* In the constructor (`__init__`) we define all layers that our network will have. Those layers are stored as internal variables of the class, and PyTorch will automatically know that parameters of those layers should be optimized when training. Internally, PyTorch uses `parameters()` method to look for all trainable parameters, and `nn.Module` will automatically collect all trainable parameters from all sub-modules. \n",
    "* We define the `forward` method that does the forward pass computation of our neural network. In our case, we start with a parameter tensor `x`, and explicitly pass it through all the layers and activation functions, starting from `flatten`, up to final linear layer `out`.\n",
    "\n",
    "To use the model, we pass it the input data `x` by writing `out = net(x)`. This executes the model's `forward`,\n",
    "along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n",
    "\n",
    "\n",
    "In fact, `Sequential` networks are represented in a very similar manner, they just store a list of layers and apply them sequentially during the forward pass. Here we have a chance to represent this process more explicitly, which eventually gives us more flexibility. That is one of the reasons that using classes for neural network definition is a recommended and perfered practice.\n",
    "\n",
    "Now we will train our network and make sure we get similar results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0, Train acc=0.815, Val acc=0.847, Train loss=0.526, Val loss=0.422\n",
      "Epoch  1, Train acc=0.860, Val acc=0.859, Train loss=0.392, Val loss=0.383\n",
      "Epoch  2, Train acc=0.873, Val acc=0.865, Train loss=0.351, Val loss=0.363\n",
      "Epoch  3, Train acc=0.882, Val acc=0.866, Train loss=0.324, Val loss=0.359\n",
      "Epoch  4, Train acc=0.888, Val acc=0.871, Train loss=0.306, Val loss=0.355\n",
      "Epoch  5, Train acc=0.893, Val acc=0.867, Train loss=0.289, Val loss=0.372\n",
      "Epoch  6, Train acc=0.897, Val acc=0.871, Train loss=0.276, Val loss=0.361\n",
      "Epoch  7, Train acc=0.901, Val acc=0.873, Train loss=0.263, Val loss=0.358\n",
      "Epoch  8, Train acc=0.907, Val acc=0.877, Train loss=0.252, Val loss=0.352\n",
      "Epoch  9, Train acc=0.909, Val acc=0.876, Train loss=0.243, Val loss=0.370\n"
     ]
    }
   ],
   "source": [
    "hist = train_val(net, train_loader, val_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only modules which contains weights which you want to be trained during the training process should be defined in your `__init__` method.\n",
    "You don't need do define activation functions like softmax, ReLU or sigmoid in your `__init__`, you can just call them in forward.\n",
    "\n",
    "If you define, say, a Linear layer in the forward function of your model, then the scope of this \"layer\" and its trainable parameters is local to the function and will be discarded after every call to the forward method. You cannot update and train weights that are constantly being discarded after every forward pass.\n",
    "However, when the Linear layer is a member of your model its scope extends beyond the forward method and the trainable parameters persists as long as the model object exists. This way you can update and train the model and its weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Example\n",
    "Say, we want to classify images of ants and bees with a small dataset. ([Download](https://www.kaggle.com/gauravduttakiit/ants-bees) the dataset from Kaggle)\n",
    "\n",
    "Training big models from scratch require lot of data to make sure that\n",
    "the training signal gets to all parts of the architecture and trains them significantly. But we have only a small dataset. Can we look for pre-trained models? Indeed, Pytorch makes available many famous pre-trained models like Alexnet, VGGnet, Resnet etc.. But these models have been trained on Imagenet with 1000 classes. Here is where transfer learning comes in. We take a pre-trained model and slightly alter the architecture to include a new network block at the end-two outputs instead of 1000\n",
    "\n",
    "In transfer learning, we fine-tune a pretrained model with a much smaller dataset of new images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did earlier,we’ll leverage functions from the Torchvision library for\n",
    "creating the datasets, loading the data, and applying the data\n",
    "transforms. First we'll define our transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms = transforms.Compose([\n",
    "    #transforms.RandomResizedCrop(224),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456,0.406], \n",
    "        [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_dataset = datasets.ImageFolder(\n",
    "          root='./hymenoptera_data/train',\n",
    "          transform=transforms)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "            root='./hymenoptera_data/val',\n",
    "            transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code we used `ImageFolder` dataset to pull\n",
    "images from our data folders and set the transforms to the ones\n",
    "we defined earlier. Next, we define our dataloaders for batch\n",
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=4,\n",
    "            shuffle=True\n",
    "            )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=4,\n",
    "            shuffle=True \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset), len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pretrained model:\n",
    "\n",
    "Torchvision provides many famous pretrained models for\n",
    "computer vision and image processing [here](https://pytorch.org/vision/main/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "#transfer_model = models.efficientnet_b0(pretrained=True)\n",
    "transfer_model = models.resnet18(pretrained=True)\n",
    "print(transfer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to freeze the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in transfer_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "transfer_model.fc = Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, train_loader, val_loader, optimizer = None, lr = 0.001, n_epochs=5, loss_fn = nn.NLLLoss()):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optimizer or torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    res = { 'train_loss' : [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training \n",
    "        model.train()\n",
    "        running_loss , num_correct = 0.0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _,predicted = torch.max(outputs,1)\n",
    "            num_correct += torch.sum((predicted == labels)).item()     \n",
    "              \n",
    "        #scheduler.step()\n",
    "        train_loss = running_loss/len(train_loader.dataset) \n",
    "        train_acc = num_correct/len(train_loader.dataset) \n",
    "        res['train_loss'].append(train_loss)        \n",
    "        res['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss, num_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _,predicted = torch.max(outputs,1)\n",
    "                num_correct += (predicted == labels).sum().item()            \n",
    "\n",
    "            val_loss = running_loss/len(val_loader.dataset) \n",
    "            val_acc = num_correct/len(val_loader.dataset) \n",
    "            res['val_loss'].append(val_loss)        \n",
    "            res['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:2}, Train acc={train_acc:.3f}, Val acc={val_acc:.3f}, Train loss={train_loss:.3f}, Val loss={val_loss:.3f}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "                    transfer_model.parameters(), \n",
    "                    lr=0.001)\n",
    "\n",
    "history=train_val(transfer_model, train_loader,val_loader, optimizer=optimizer, n_epochs=10, loss_fn = loss_fn)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79d33c79908047a752ed9470bb2cb08a4a794c56781a87ef3b000ffb38436d43"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
